{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE': 384,\n",
    "    'BATCH_SIZE': 16,\n",
    "    'EPOCHS': 12,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED' : 42,\n",
    "    'N_SPLITS': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            # 테스트셋: 라벨 없이 이미지 경로만 저장\n",
    "            for fname in sorted(os.listdir(root_dir)):\n",
    "                if fname.lower().endswith(('.jpg')):\n",
    "                    img_path = os.path.join(root_dir, fname)\n",
    "                    self.samples.append((img_path,))\n",
    "        else:\n",
    "            # 학습셋: 클래스별 폴더 구조에서 라벨 추출\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "            for cls_name in self.classes:\n",
    "                cls_folder = os.path.join(root_dir, cls_name)\n",
    "\n",
    "                if not os.path.isdir(cls_folder):\n",
    "                    continue\n",
    "                \n",
    "                for fname in os.listdir(cls_folder):\n",
    "                    if fname.lower().endswith(('.jpg')):\n",
    "                        img_path = os.path.join(cls_folder, fname)\n",
    "                        label = self.class_to_idx[cls_name]\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.samples[idx][0]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = './train'\n",
    "test_root = './test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'] + 32, CFG['IMG_SIZE'] + 32)),  # 약간 크게 리사이즈 후\n",
    "    transforms.RandomResizedCrop(CFG['IMG_SIZE'], scale=(0.8, 1.0)),  # 랜덤 크롭\n",
    "    transforms.RandomHorizontalFlip(),                                # 좌우 뒤집기\n",
    "    transforms.RandomRotation(10),                                    # ±10도 회전\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 색상 변형\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 이미지 수: 33137\n",
      "train 이미지 수: 26509, valid 이미지 수: 6628\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터셋 로드\n",
    "full_dataset = CustomImageDataset(train_root, transform=None)\n",
    "print(f\"총 이미지 수: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "# Stratified Split\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "# Subset + transform 각각 적용\n",
    "train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "print(f'train 이미지 수: {len(train_dataset)}, valid 이미지 수: {len(val_dataset)}')\n",
    "\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)  # ResNet18 모델 불러오기\n",
    "        self.feature_dim = self.backbone.fc.in_features \n",
    "        self.backbone.fc = nn.Identity()  # feature extractor로만 사용\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)  # 분류기\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)       \n",
    "        x = self.head(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        # EfficientNet-b3 백본\n",
    "        self.backbone = timm.create_model('efficientnet_b3', pretrained=True)\n",
    "        \n",
    "        # 기존 classifier 제거\n",
    "        self.feature_dim = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # 새 분류기\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNetV2-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    " \n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        # EfficientNet-b3 백본\n",
    "        self.backbone = timm.create_model('efficientnetv2_rw_m', pretrained=True)\n",
    "        \n",
    "        # 기존 classifier 제거\n",
    "        self.feature_dim = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # 새 분류기\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1/15] Training: 100%|██████████| 1657/1657 [28:34<00:00,  1.03s/it]\n",
      "[Epoch 1/15] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.88it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 Learning Rate after epoch 1: 0.00009892\n",
      "Train Loss : 3.1939 || Valid Loss : 0.6222 | Valid Accuracy : 85.9234%\n",
      "📦 Best model saved at epoch 1 (logloss: 0.6222)\n",
      "📦 Best model saved at epoch 1 (val_acc: 85.9234)\n",
      "📦 Best model saved at epoch 1 (val_loss: 0.6222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2/15] Training:  43%|████▎     | 711/1657 [12:24<17:28,  1.11s/it]"
     ]
    }
   ],
   "source": [
    "model = BaseModel(num_classes=len(class_names)).to(device)\n",
    "best_logloss = float('inf')\n",
    "best_acc = 0.0\n",
    "best_ce_loss = float('inf')\n",
    "\n",
    "# 손실 함수\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=CFG['EPOCHS'], eta_min=1e-6\n",
    ")\n",
    "\n",
    "# 학습 및 검증 루프\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        inputs, targets_a, targets_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # LogLoss\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"📉 Learning Rate after epoch {epoch+1}: {scheduler.get_last_lr()[0]:.8f}\")\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | Valid Accuracy : {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Best model 저장\n",
    "    if val_logloss < best_logloss:\n",
    "        best_logloss = val_logloss\n",
    "        torch.save(model.state_dict(), f'best_logloss.pth')\n",
    "        print(f\"📦 Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")\n",
    "\n",
    "    if val_accuracy > best_acc:\n",
    "        best_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), \"best_acc.pth\")\n",
    "        print(f\"📦 Best model saved at epoch {epoch+1} (val_acc: {val_accuracy:.4f})\")\n",
    "\n",
    "    if avg_val_loss < best_ce_loss:\n",
    "        best_ce_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_loss.pth\")\n",
    "        print(f\"📦 Best model saved at epoch {epoch+1} (val_loss: {avg_val_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 1/12] Training: 100%|██████████| 1657/1657 [28:43<00:00,  1.04s/it]\n",
      "[Fold 1][Epoch 1/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1081 | Val Loss: 0.6150 | Val Acc: 85.92% | LogLoss: 0.6151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 2/12] Training: 100%|██████████| 1657/1657 [28:25<00:00,  1.03s/it]\n",
      "[Fold 1][Epoch 2/12] Validation: 100%|██████████| 415/415 [01:48<00:00,  3.83it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0496 | Val Loss: 0.2973 | Val Acc: 91.55% | LogLoss: 0.2976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 3/12] Training: 100%|██████████| 1657/1657 [28:23<00:00,  1.03s/it]\n",
      "[Fold 1][Epoch 3/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8294 | Val Loss: 0.1871 | Val Acc: 94.07% | LogLoss: 0.1874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 4/12] Training: 100%|██████████| 1657/1657 [27:51<00:00,  1.01s/it]\n",
      "[Fold 1][Epoch 4/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.92it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7091 | Val Loss: 0.1604 | Val Acc: 94.86% | LogLoss: 0.1607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 5/12] Training: 100%|██████████| 1657/1657 [27:50<00:00,  1.01s/it]\n",
      "[Fold 1][Epoch 5/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6577 | Val Loss: 0.1362 | Val Acc: 95.67% | LogLoss: 0.1365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 6/12] Training: 100%|██████████| 1657/1657 [27:49<00:00,  1.01s/it]\n",
      "[Fold 1][Epoch 6/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5926 | Val Loss: 0.1255 | Val Acc: 96.23% | LogLoss: 0.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 7/12] Training: 100%|██████████| 1657/1657 [27:49<00:00,  1.01s/it]\n",
      "[Fold 1][Epoch 7/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.92it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5360 | Val Loss: 0.1010 | Val Acc: 97.01% | LogLoss: 0.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 8/12] Training: 100%|██████████| 1657/1657 [27:52<00:00,  1.01s/it]\n",
      "[Fold 1][Epoch 8/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.93it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5124 | Val Loss: 0.0977 | Val Acc: 96.98% | LogLoss: 0.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 9/12] Training: 100%|██████████| 1657/1657 [27:50<00:00,  1.01s/it]\n",
      "[Fold 1][Epoch 9/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.90it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4684 | Val Loss: 0.1020 | Val Acc: 96.97% | LogLoss: 0.1021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 10/12] Training: 100%|██████████| 1657/1657 [27:49<00:00,  1.01s/it]\n",
      "[Fold 1][Epoch 10/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.93it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4408 | Val Loss: 0.0938 | Val Acc: 97.30% | LogLoss: 0.0940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 11/12] Training: 100%|██████████| 1657/1657 [27:50<00:00,  1.01s/it]\n",
      "[Fold 1][Epoch 11/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.92it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4468 | Val Loss: 0.0926 | Val Acc: 97.31% | LogLoss: 0.0928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 12/12] Training: 100%|██████████| 1657/1657 [27:49<00:00,  1.01s/it]\n",
      "[Fold 1][Epoch 12/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.93it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4134 | Val Loss: 0.0892 | Val Acc: 97.56% | LogLoss: 0.0894\n",
      "\n",
      "📂 Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 1/12] Training: 100%|██████████| 1657/1657 [30:52<00:00,  1.12s/it]\n",
      "[Fold 2][Epoch 1/12] Validation: 100%|██████████| 415/415 [01:47<00:00,  3.86it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1314 | Val Loss: 0.5619 | Val Acc: 85.82% | LogLoss: 0.5627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 2/12] Training: 100%|██████████| 1657/1657 [30:22<00:00,  1.10s/it]\n",
      "[Fold 2][Epoch 2/12] Validation: 100%|██████████| 415/415 [01:47<00:00,  3.86it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0476 | Val Loss: 0.2914 | Val Acc: 91.57% | LogLoss: 0.2919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 3/12] Training: 100%|██████████| 1657/1657 [30:23<00:00,  1.10s/it]\n",
      "[Fold 2][Epoch 3/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8590 | Val Loss: 0.2338 | Val Acc: 93.75% | LogLoss: 0.2340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 4/12] Training: 100%|██████████| 1657/1657 [30:28<00:00,  1.10s/it]\n",
      "[Fold 2][Epoch 4/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.94it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7227 | Val Loss: 0.1671 | Val Acc: 95.04% | LogLoss: 0.1673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 5/12] Training: 100%|██████████| 1657/1657 [30:25<00:00,  1.10s/it]\n",
      "[Fold 2][Epoch 5/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6210 | Val Loss: 0.1657 | Val Acc: 95.25% | LogLoss: 0.1660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 6/12] Training: 100%|██████████| 1657/1657 [30:23<00:00,  1.10s/it]\n",
      "[Fold 2][Epoch 6/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.90it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5800 | Val Loss: 0.1371 | Val Acc: 95.94% | LogLoss: 0.1373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 7/12] Training: 100%|██████████| 1657/1657 [30:26<00:00,  1.10s/it]\n",
      "[Fold 2][Epoch 7/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.93it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5251 | Val Loss: 0.1444 | Val Acc: 96.06% | LogLoss: 0.1446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 8/12] Training: 100%|██████████| 1657/1657 [30:25<00:00,  1.10s/it]\n",
      "[Fold 2][Epoch 8/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.90it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5063 | Val Loss: 0.1232 | Val Acc: 96.56% | LogLoss: 0.1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 9/12] Training: 100%|██████████| 1657/1657 [30:40<00:00,  1.11s/it]\n",
      "[Fold 2][Epoch 9/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.88it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4905 | Val Loss: 0.1181 | Val Acc: 96.97% | LogLoss: 0.1182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 10/12] Training: 100%|██████████| 1657/1657 [32:29<00:00,  1.18s/it]\n",
      "[Fold 2][Epoch 10/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4453 | Val Loss: 0.1072 | Val Acc: 97.09% | LogLoss: 0.1073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 11/12] Training: 100%|██████████| 1657/1657 [30:41<00:00,  1.11s/it]\n",
      "[Fold 2][Epoch 11/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.93it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4649 | Val Loss: 0.1069 | Val Acc: 97.22% | LogLoss: 0.1071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 12/12] Training: 100%|██████████| 1657/1657 [30:29<00:00,  1.10s/it]\n",
      "[Fold 2][Epoch 12/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.90it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4271 | Val Loss: 0.1025 | Val Acc: 97.28% | LogLoss: 0.1027\n",
      "\n",
      "📂 Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 1/12] Training: 100%|██████████| 1657/1657 [36:07<00:00,  1.31s/it]\n",
      "[Fold 3][Epoch 1/12] Validation: 100%|██████████| 415/415 [01:47<00:00,  3.88it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0946 | Val Loss: 0.5967 | Val Acc: 84.52% | LogLoss: 0.5977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 2/12] Training: 100%|██████████| 1657/1657 [35:51<00:00,  1.30s/it]\n",
      "[Fold 3][Epoch 2/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0704 | Val Loss: 0.2753 | Val Acc: 92.20% | LogLoss: 0.2757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 3/12] Training: 100%|██████████| 1657/1657 [36:01<00:00,  1.30s/it]\n",
      "[Fold 3][Epoch 3/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8343 | Val Loss: 0.1870 | Val Acc: 94.42% | LogLoss: 0.1873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 4/12] Training: 100%|██████████| 1657/1657 [35:39<00:00,  1.29s/it]\n",
      "[Fold 3][Epoch 4/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.90it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6890 | Val Loss: 0.1734 | Val Acc: 95.16% | LogLoss: 0.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 5/12] Training: 100%|██████████| 1657/1657 [37:23<00:00,  1.35s/it] \n",
      "[Fold 3][Epoch 5/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.90it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6420 | Val Loss: 0.1516 | Val Acc: 95.76% | LogLoss: 0.1519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 6/12] Training: 100%|██████████| 1657/1657 [35:54<00:00,  1.30s/it]\n",
      "[Fold 3][Epoch 6/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.90it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6082 | Val Loss: 0.1474 | Val Acc: 96.02% | LogLoss: 0.1477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 7/12] Training: 100%|██████████| 1657/1657 [36:53<00:00,  1.34s/it] \n",
      "[Fold 3][Epoch 7/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.90it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5310 | Val Loss: 0.1226 | Val Acc: 96.68% | LogLoss: 0.1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 8/12] Training: 100%|██████████| 1657/1657 [36:47<00:00,  1.33s/it] \n",
      "[Fold 3][Epoch 8/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.93it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4970 | Val Loss: 0.1095 | Val Acc: 97.19% | LogLoss: 0.1097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 9/12] Training: 100%|██████████| 1657/1657 [35:47<00:00,  1.30s/it]\n",
      "[Fold 3][Epoch 9/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4544 | Val Loss: 0.1073 | Val Acc: 97.27% | LogLoss: 0.1075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 10/12] Training: 100%|██████████| 1657/1657 [35:33<00:00,  1.29s/it]\n",
      "[Fold 3][Epoch 10/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.90it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4452 | Val Loss: 0.1069 | Val Acc: 97.45% | LogLoss: 0.1071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 11/12] Training: 100%|██████████| 1657/1657 [37:08<00:00,  1.34s/it]\n",
      "[Fold 3][Epoch 11/12] Validation: 100%|██████████| 415/415 [01:53<00:00,  3.66it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4402 | Val Loss: 0.0984 | Val Acc: 97.66% | LogLoss: 0.0986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 12/12] Training: 100%|██████████| 1657/1657 [38:19<00:00,  1.39s/it] \n",
      "[Fold 3][Epoch 12/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.90it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4283 | Val Loss: 0.0996 | Val Acc: 97.66% | LogLoss: 0.0998\n",
      "\n",
      "📂 Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 1/12] Training: 100%|██████████| 1657/1657 [36:48<00:00,  1.33s/it] \n",
      "[Fold 4][Epoch 1/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.88it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0139 | Val Loss: 0.5064 | Val Acc: 86.04% | LogLoss: 0.5072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 2/12] Training: 100%|██████████| 1657/1657 [33:39<00:00,  1.22s/it]\n",
      "[Fold 4][Epoch 2/12] Validation: 100%|██████████| 415/415 [01:48<00:00,  3.81it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0532 | Val Loss: 0.2843 | Val Acc: 92.47% | LogLoss: 0.2847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 3/12] Training: 100%|██████████| 1657/1657 [33:45<00:00,  1.22s/it]\n",
      "[Fold 4][Epoch 3/12] Validation: 100%|██████████| 415/415 [01:48<00:00,  3.82it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8267 | Val Loss: 0.1924 | Val Acc: 94.05% | LogLoss: 0.1928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 4/12] Training: 100%|██████████| 1657/1657 [34:06<00:00,  1.23s/it] \n",
      "[Fold 4][Epoch 4/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7201 | Val Loss: 0.1930 | Val Acc: 94.54% | LogLoss: 0.1934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 5/12] Training: 100%|██████████| 1657/1657 [33:31<00:00,  1.21s/it]\n",
      "[Fold 4][Epoch 5/12] Validation: 100%|██████████| 415/415 [01:47<00:00,  3.87it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6600 | Val Loss: 0.1558 | Val Acc: 95.19% | LogLoss: 0.1560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 6/12] Training: 100%|██████████| 1657/1657 [36:02<00:00,  1.31s/it]\n",
      "[Fold 4][Epoch 6/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5780 | Val Loss: 0.1365 | Val Acc: 95.90% | LogLoss: 0.1367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 7/12] Training: 100%|██████████| 1657/1657 [33:39<00:00,  1.22s/it]\n",
      "[Fold 4][Epoch 7/12] Validation: 100%|██████████| 415/415 [01:45<00:00,  3.92it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5428 | Val Loss: 0.1295 | Val Acc: 96.39% | LogLoss: 0.1298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 8/12] Training: 100%|██████████| 1657/1657 [35:06<00:00,  1.27s/it]\n",
      "[Fold 4][Epoch 8/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4962 | Val Loss: 0.1282 | Val Acc: 96.59% | LogLoss: 0.1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 9/12] Training: 100%|██████████| 1657/1657 [33:41<00:00,  1.22s/it]\n",
      "[Fold 4][Epoch 9/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4727 | Val Loss: 0.1174 | Val Acc: 96.80% | LogLoss: 0.1176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 10/12] Training: 100%|██████████| 1657/1657 [33:54<00:00,  1.23s/it]\n",
      "[Fold 4][Epoch 10/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.88it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4387 | Val Loss: 0.1086 | Val Acc: 97.07% | LogLoss: 0.1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 11/12] Training: 100%|██████████| 1657/1657 [34:06<00:00,  1.24s/it]\n",
      "[Fold 4][Epoch 11/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4266 | Val Loss: 0.1113 | Val Acc: 96.98% | LogLoss: 0.1115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 4][Epoch 12/12] Training: 100%|██████████| 1657/1657 [33:43<00:00,  1.22s/it] \n",
      "[Fold 4][Epoch 12/12] Validation: 100%|██████████| 415/415 [01:46<00:00,  3.91it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4222 | Val Loss: 0.1079 | Val Acc: 96.97% | LogLoss: 0.1081\n",
      "\n",
      "📂 Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 5][Epoch 1/12] Training: 100%|██████████| 1657/1657 [36:27<00:00,  1.32s/it]\n",
      "[Fold 5][Epoch 1/12] Validation: 100%|██████████| 415/415 [01:47<00:00,  3.87it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     66\u001b[39m avg_val_loss = val_loss / \u001b[38;5;28mlen\u001b[39m(val_loader)\n\u001b[32m     67\u001b[39m val_accuracy = \u001b[32m100\u001b[39m * correct / total\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m val_logloss = \u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m scheduler.step()\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% | LogLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_logloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/torch_venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2956\u001b[39m, in \u001b[36mlog_loss\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight, labels)\u001b[39m\n\u001b[32m   2878\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m   2879\u001b[39m     {\n\u001b[32m   2880\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   2887\u001b[39m )\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_loss\u001b[39m(y_true, y_pred, *, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, labels=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2889\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Log loss, aka logistic loss or cross-entropy loss.\u001b[39;00m\n\u001b[32m   2890\u001b[39m \n\u001b[32m   2891\u001b[39m \u001b[33;03m    This is the loss function used in (multinomial) logistic regression\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2954\u001b[39m \u001b[33;03m    0.21616...\u001b[39;00m\n\u001b[32m   2955\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2956\u001b[39m     y_pred = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2957\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2960\u001b[39m     check_consistent_length(y_pred, y_true, sample_weight)\n\u001b[32m   2961\u001b[39m     lb = LabelBinarizer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/torch_venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/torch_venv/lib/python3.12/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/torch_venv/lib/python3.12/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# === K-Fold 설정 ===\n",
    "skf = StratifiedKFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=42)\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n",
    "    print(f\"\\n📂 Fold {fold+1}/{CFG['N_SPLITS']}\")\n",
    "\n",
    "    train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "    val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    # === 모델, 옵티마이저, 스케줄러, 손실 함수 설정 ===\n",
    "    model = BaseModel(num_classes=len(class_names)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'], eta_min=1e-6)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_logloss = float('inf')\n",
    "    best_acc = 0.0\n",
    "    best_ce_loss = float('inf')\n",
    "\n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"[Fold {fold+1}][Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"[Fold {fold+1}][Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | LogLoss: {val_logloss:.4f}\")\n",
    "\n",
    "        # === 모델 저장 ===\n",
    "        save_dir = f\"model_fold{fold+1}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        if val_logloss < best_logloss:\n",
    "            best_logloss = val_logloss\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/best_logloss.pth\")\n",
    "\n",
    "        if val_accuracy > best_acc:\n",
    "            best_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/best_acc.pth\")\n",
    "\n",
    "        if avg_val_loss < best_ce_loss:\n",
    "            best_ce_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/best_loss.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset(test_root, transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 로드\n",
    "model = BaseModel(num_classes=len(class_names))\n",
    "model.load_state_dict(torch.load('best_logloss.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # 각 배치의 확률을 리스트로 변환\n",
    "        for prob in probs.cpu():  # prob: (num_classes,)\n",
    "            result = {\n",
    "                class_names[i]: prob[i].item()\n",
    "                for i in range(len(class_names))\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "pred = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "all_fold_probs = []\n",
    "\n",
    "for fold in range(1, 5):  # Fold 1~4\n",
    "    model = BaseModel(num_classes=len(class_names))\n",
    "    model.load_state_dict(torch.load(f\"model_fold{fold}/best_logloss.pth\", map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    fold_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            fold_probs.append(probs.cpu().numpy())\n",
    "\n",
    "    fold_probs = np.concatenate(fold_probs, axis=0)  # (N, C)\n",
    "    all_fold_probs.append(fold_probs)\n",
    "\n",
    "# 평균 앙상블\n",
    "ensemble_probs = np.mean(np.stack(all_fold_probs), axis=0)\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(ensemble_probs, columns=class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 'ID' 컬럼을 제외한 클래스 컬럼 정렬\n",
    "class_columns = submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "\n",
    "submission[class_columns] = pred.values\n",
    "submission.to_csv('best_logloss_submission.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_logloss_submission.csv 저장 완료\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "class_columns = submission.columns[1:]  # ID 제외한 클래스 열들\n",
    "pred_df = pred_df[class_columns]\n",
    "\n",
    "submission[class_columns] = pred_df.values\n",
    "submission.to_csv('best_logloss_submission.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"best_logloss_submission.csv 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'] + 32, CFG['IMG_SIZE'] + 32)),  # 크게 리사이즈 후\n",
    "    transforms.FiveCrop(CFG['IMG_SIZE']),  # (top-left, top-right, bottom-left, bottom-right, center)\n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "    transforms.Lambda(lambda crops: torch.stack([\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])(crop) for crop in crops\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTADataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        crops = self.transform(image)  # (5, 3, H, W)\n",
    "        return crops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_dir = './test'\n",
    "test_image_paths = sorted([os.path.join(test_image_dir, fname) for fname in os.listdir(test_image_dir)])\n",
    "\n",
    "test_dataset = TTADataset(test_image_paths, tta_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Inference with Fold 1 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517/517 [16:16<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Inference with Fold 2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517/517 [14:34<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Inference with Fold 3 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517/517 [13:20<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Inference with Fold 4 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517/517 [13:07<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 제출 파일 저장 완료: tta_4fold_submission.csv\n"
     ]
    }
   ],
   "source": [
    "all_fold_probs = []\n",
    "\n",
    "for fold in range(1, 5):  # model_fold1 ~ model_fold4\n",
    "    print(f\"📂 Inference with Fold {fold} model...\")\n",
    "\n",
    "    # === 모델 로드 ===\n",
    "    model = BaseModel(num_classes=len(class_names))\n",
    "    model.load_state_dict(torch.load(f\"model_fold{fold}/best_logloss.pth\", map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    fold_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_crops in tqdm(test_loader):\n",
    "            B, N, C, H, W = batch_crops.shape  # (B, 5, C, H, W)\n",
    "            batch_crops = batch_crops.view(-1, C, H, W).to(device)  # (B×5, C, H, W)\n",
    "\n",
    "            outputs = model(batch_crops)\n",
    "            probs = F.softmax(outputs, dim=1).view(B, N, -1)  # (B, 5, num_classes)\n",
    "            avg_probs = probs.mean(dim=1)  # (B, num_classes)\n",
    "\n",
    "            fold_probs.append(avg_probs.cpu().numpy())\n",
    "\n",
    "    fold_probs = np.concatenate(fold_probs, axis=0)  # (전체 test 이미지 수, num_classes)\n",
    "    all_fold_probs.append(fold_probs)\n",
    "\n",
    "# ✅ 최종 앙상블 (soft voting)\n",
    "final_probs = np.mean(np.stack(all_fold_probs), axis=0)\n",
    "pred_df = pd.DataFrame(final_probs, columns=class_names)\n",
    "\n",
    "# ✅ sample_submission 기반 저장\n",
    "submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')\n",
    "class_columns = submission.columns[1:]\n",
    "submission[class_columns] = pred_df[class_columns].values\n",
    "submission.to_csv('tta_4fold_submission.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"✅ 제출 파일 저장 완료: tta_4fold_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
