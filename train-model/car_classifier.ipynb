{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE': 384,\n",
    "    'BATCH_SIZE': 16,\n",
    "    'EPOCHS': 15,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED' : 42,\n",
    "    'N_SPLITS': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            for fname in sorted(os.listdir(root_dir)):\n",
    "                if fname.lower().endswith(('.jpg')):\n",
    "                    img_path = os.path.join(root_dir, fname)\n",
    "                    self.samples.append((img_path,))\n",
    "        else:\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "            for cls_name in self.classes:\n",
    "                cls_folder = os.path.join(root_dir, cls_name)\n",
    "\n",
    "                if not os.path.isdir(cls_folder):\n",
    "                    continue\n",
    "                \n",
    "                for fname in os.listdir(cls_folder):\n",
    "                    if fname.lower().endswith(('.jpg')):\n",
    "                        img_path = os.path.join(cls_folder, fname)\n",
    "                        label = self.class_to_idx[cls_name]\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.samples[idx][0]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = './filtered_train'\n",
    "test_root = './test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'] + 32, CFG['IMG_SIZE'] + 32)),\n",
    "    transforms.RandomResizedCrop(CFG['IMG_SIZE'], scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï¥ù Ïù¥ÎØ∏ÏßÄ Ïàò: 33131\n",
      "train Ïù¥ÎØ∏ÏßÄ Ïàò: 26504, valid Ïù¥ÎØ∏ÏßÄ Ïàò: 6627\n"
     ]
    }
   ],
   "source": [
    "full_dataset = CustomImageDataset(train_root, transform=None)\n",
    "print(f\"Ï¥ù Ïù¥ÎØ∏ÏßÄ Ïàò: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "print(f'train Ïù¥ÎØ∏ÏßÄ Ïàò: {len(train_dataset)}, valid Ïù¥ÎØ∏ÏßÄ Ïàò: {len(val_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model('convnext_base_384_in22ft1k', pretrained=True, features_only=False)\n",
    "        self.feature_dim = self.backbone.head.in_features\n",
    "        self.backbone.head = nn.Identity()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.forward_features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size, _, H, W = x.size()\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    cut_w = int(W * np.sqrt(1 - lam))\n",
    "    cut_h = int(H * np.sqrt(1 - lam))\n",
    "\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]\n",
    "    y_a, y_b = y, y[index]\n",
    "    lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def apply_mixup_or_cutmix(x, y, mix_prob=0.5, mixup_alpha=0.2, cutmix_alpha=1.0):\n",
    "    if np.random.rand() < mix_prob:\n",
    "        return mixup_data(x, y, alpha=mixup_alpha)\n",
    "    else:\n",
    "        return cutmix_data(x, y, alpha=cutmix_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name convnext_base_384_in22ft1k to current convnext_base.fb_in22k_ft_in1k_384.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Resuming fold 1 from final_model_fold1/checkpoint_epoch_005.pth\n",
      "Resume: start_epoch=5, best_logloss=0.15354506109472799, best_acc=95.19195943498733, best_ce_loss=0.15338741486953428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 6/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [38:40<00:00,  1.68s/it]\n",
      "[Fold 1][Epoch 6/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:29<00:00,  2.10it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9392 | Val Loss: 0.1390 | Val Acc: 95.93% | LogLoss: 0.1392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 7/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [38:14<00:00,  1.66s/it]\n",
      "[Fold 1][Epoch 7/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:26<00:00,  2.11it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8840 | Val Loss: 0.1104 | Val Acc: 96.44% | LogLoss: 0.1105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 8/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [37:51<00:00,  1.65s/it]\n",
      "[Fold 1][Epoch 8/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:22<00:00,  2.14it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8386 | Val Loss: 0.1110 | Val Acc: 96.57% | LogLoss: 0.1111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 9/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [37:34<00:00,  1.63s/it]\n",
      "[Fold 1][Epoch 9/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:21<00:00,  2.15it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7874 | Val Loss: 0.1048 | Val Acc: 96.90% | LogLoss: 0.1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 10/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [37:50<00:00,  1.64s/it]\n",
      "[Fold 1][Epoch 10/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:26<00:00,  2.11it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7301 | Val Loss: 0.0960 | Val Acc: 97.08% | LogLoss: 0.0961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 11/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [38:53<00:00,  1.69s/it]\n",
      "[Fold 1][Epoch 11/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:41<00:00,  2.02it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7042 | Val Loss: 0.0884 | Val Acc: 97.32% | LogLoss: 0.0885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 12/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [39:01<00:00,  1.70s/it]\n",
      "[Fold 1][Epoch 12/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:37<00:00,  2.05it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6850 | Val Loss: 0.0866 | Val Acc: 97.35% | LogLoss: 0.0867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 13/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [37:51<00:00,  1.64s/it]\n",
      "[Fold 1][Epoch 13/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:23<00:00,  2.14it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6648 | Val Loss: 0.0858 | Val Acc: 97.41% | LogLoss: 0.0859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 14/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [37:33<00:00,  1.63s/it]\n",
      "[Fold 1][Epoch 14/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:21<00:00,  2.15it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6607 | Val Loss: 0.0840 | Val Acc: 97.40% | LogLoss: 0.0841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 1][Epoch 15/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [37:22<00:00,  1.62s/it]\n",
      "[Fold 1][Epoch 15/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:17<00:00,  2.18it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6621 | Val Loss: 0.0841 | Val Acc: 97.41% | LogLoss: 0.0842\n",
      "\n",
      "üìÇ Fold 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name convnext_base_384_in22ft1k to current convnext_base.fb_in22k_ft_in1k_384.\n",
      "  model = create_fn(\n",
      "[Fold 2][Epoch 1/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [38:58<00:00,  1.69s/it]\n",
      "[Fold 2][Epoch 1/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:17<00:00,  2.18it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.0099 | Val Loss: 0.5538 | Val Acc: 84.76% | LogLoss: 0.5544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 2/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [41:37<00:00,  1.81s/it]\n",
      "[Fold 2][Epoch 2/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:44<00:00,  2.01it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5564 | Val Loss: 0.2685 | Val Acc: 91.32% | LogLoss: 0.2688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 3/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [42:35<00:00,  1.85s/it]\n",
      "[Fold 2][Epoch 3/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:49<00:00,  1.98it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2858 | Val Loss: 0.2244 | Val Acc: 93.63% | LogLoss: 0.2246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 4/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [42:22<00:00,  1.84s/it]\n",
      "[Fold 2][Epoch 4/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:46<00:00,  2.00it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1111 | Val Loss: 0.1727 | Val Acc: 94.62% | LogLoss: 0.1728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 5/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [41:51<00:00,  1.82s/it]\n",
      "[Fold 2][Epoch 5/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:51<00:00,  1.97it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0052 | Val Loss: 0.1490 | Val Acc: 95.39% | LogLoss: 0.1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 6/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [42:25<00:00,  1.84s/it]\n",
      "[Fold 2][Epoch 6/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:50<00:00,  1.97it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9495 | Val Loss: 0.1268 | Val Acc: 96.05% | LogLoss: 0.1269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 7/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [42:01<00:00,  1.83s/it]\n",
      "[Fold 2][Epoch 7/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:42<00:00,  2.02it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9023 | Val Loss: 0.1156 | Val Acc: 96.35% | LogLoss: 0.1157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 8/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [42:25<00:00,  1.84s/it]\n",
      "[Fold 2][Epoch 8/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:55<00:00,  1.94it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8302 | Val Loss: 0.1117 | Val Acc: 96.65% | LogLoss: 0.1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 9/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [43:31<00:00,  1.89s/it]\n",
      "[Fold 2][Epoch 9/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:49<00:00,  1.97it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7989 | Val Loss: 0.1017 | Val Acc: 96.89% | LogLoss: 0.1018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 10/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [43:00<00:00,  1.87s/it]\n",
      "[Fold 2][Epoch 10/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:57<00:00,  1.93it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7254 | Val Loss: 0.0975 | Val Acc: 97.08% | LogLoss: 0.0976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 11/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [42:46<00:00,  1.86s/it]\n",
      "[Fold 2][Epoch 11/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:52<00:00,  1.96it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7333 | Val Loss: 0.0891 | Val Acc: 97.21% | LogLoss: 0.0892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 12/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [42:28<00:00,  1.85s/it]\n",
      "[Fold 2][Epoch 12/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:52<00:00,  1.96it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6956 | Val Loss: 0.0855 | Val Acc: 97.49% | LogLoss: 0.0856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 13/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [43:11<00:00,  1.88s/it]\n",
      "[Fold 2][Epoch 13/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:45<00:00,  2.00it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6735 | Val Loss: 0.0820 | Val Acc: 97.43% | LogLoss: 0.0820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 14/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [41:50<00:00,  1.82s/it]\n",
      "[Fold 2][Epoch 14/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:45<00:00,  2.00it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6844 | Val Loss: 0.0809 | Val Acc: 97.69% | LogLoss: 0.0810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 2][Epoch 15/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [42:18<00:00,  1.84s/it]\n",
      "[Fold 2][Epoch 15/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:57<00:00,  1.93it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6549 | Val Loss: 0.0805 | Val Acc: 97.65% | LogLoss: 0.0806\n",
      "\n",
      "üìÇ Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name convnext_base_384_in22ft1k to current convnext_base.fb_in22k_ft_in1k_384.\n",
      "  model = create_fn(\n",
      "[Fold 3][Epoch 1/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [44:12<00:00,  1.92s/it] \n",
      "[Fold 3][Epoch 1/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:18<00:00,  2.17it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.0209 | Val Loss: 0.6242 | Val Acc: 83.41% | LogLoss: 0.6248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 2/15] Training:   5%|‚ñå         | 70/1381 [36:45<9:18:59, 25.58s/it] "
     ]
    }
   ],
   "source": [
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_logloss = checkpoint.get('best_logloss', float('inf'))\n",
    "    best_acc = checkpoint.get('best_acc', 0.0)\n",
    "    best_ce_loss = checkpoint.get('best_ce_loss', float('inf'))\n",
    "    return start_epoch, best_logloss, best_acc, best_ce_loss\n",
    "\n",
    "resume = True\n",
    "resume_ckpt = {\n",
    "    0: \"final_model_fold1/checkpoint_epoch_005.pth\",\n",
    "    1: None,\n",
    "    2: None,\n",
    "    3: None,\n",
    "    4: None\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=42)\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n",
    "    print(f\"\\nüìÇ Fold {fold+1}/{CFG['N_SPLITS']}\")\n",
    "\n",
    "    train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "    val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "\n",
    "    model = BaseModel(num_classes=len(class_names)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'], eta_min=1e-6)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    save_dir = f\"final_model_fold{fold+1}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_logloss = float('inf')\n",
    "    best_acc = 0.0\n",
    "    best_ce_loss = float('inf')\n",
    "    if resume and resume_ckpt.get(fold):\n",
    "        print(f\"üîÑ Resuming fold {fold+1} from {resume_ckpt[fold]}\")\n",
    "        start_epoch, best_logloss, best_acc, best_ce_loss = load_checkpoint(\n",
    "            model, optimizer, scheduler, resume_ckpt[fold]\n",
    "        )\n",
    "        print(f\"Resume: start_epoch={start_epoch}, best_logloss={best_logloss}, best_acc={best_acc}, best_ce_loss={best_ce_loss}\")\n",
    "\n",
    "    for epoch in range(start_epoch, CFG['EPOCHS']):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"[Fold {fold+1}][Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            inputs, targets_a, targets_b, lam = apply_mixup_or_cutmix(\n",
    "                images, labels, mix_prob=0.5, mixup_alpha=0.2, cutmix_alpha=1.0\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"[Fold {fold+1}][Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | LogLoss: {val_logloss:.4f}\")\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_logloss': best_logloss,\n",
    "            'best_acc': best_acc,\n",
    "            'best_ce_loss': best_ce_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{save_dir}/checkpoint_epoch_{epoch+1:03d}.pth\")\n",
    "\n",
    "        if val_logloss < best_logloss:\n",
    "            best_logloss = val_logloss\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/best_logloss.pth\")\n",
    "\n",
    "        if val_accuracy > best_acc:\n",
    "            best_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/best_acc.pth\")\n",
    "\n",
    "        if avg_val_loss < best_ce_loss:\n",
    "            best_ce_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/best_loss.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name convnext_base_384_in22ft1k to current convnext_base.fb_in22k_ft_in1k_384.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Resuming fold 3 from final_model_fold3/checkpoint_epoch_001.pth\n",
      "Resume: start_epoch=1, best_logloss=inf, best_acc=0.0, best_ce_loss=inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 2/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [38:17<00:00,  1.66s/it]\n",
      "[Fold 3][Epoch 2/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:13<00:00,  2.21it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5755 | Val Loss: 0.2542 | Val Acc: 92.61% | LogLoss: 0.2544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 3/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [43:29<00:00,  1.89s/it]\n",
      "[Fold 3][Epoch 3/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:14<00:00,  2.20it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2462 | Val Loss: 0.2089 | Val Acc: 93.93% | LogLoss: 0.2091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 4/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [46:48<00:00,  2.03s/it] \n",
      "[Fold 3][Epoch 4/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:16<00:00,  2.18it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1290 | Val Loss: 0.1709 | Val Acc: 95.28% | LogLoss: 0.1711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 5/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [45:53<00:00,  1.99s/it]\n",
      "[Fold 3][Epoch 5/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:20<00:00,  2.16it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0111 | Val Loss: 0.1387 | Val Acc: 95.78% | LogLoss: 0.1388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 6/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [45:10<00:00,  1.96s/it]\n",
      "[Fold 3][Epoch 6/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:13<00:00,  2.21it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9136 | Val Loss: 0.1272 | Val Acc: 96.06% | LogLoss: 0.1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 7/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [43:59<00:00,  1.91s/it]\n",
      "[Fold 3][Epoch 7/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:15<00:00,  2.19it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8666 | Val Loss: 0.1188 | Val Acc: 96.08% | LogLoss: 0.1189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 8/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [44:44<00:00,  1.94s/it]\n",
      "[Fold 3][Epoch 8/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:16<00:00,  2.18it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8142 | Val Loss: 0.1171 | Val Acc: 96.58% | LogLoss: 0.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 9/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [47:37<00:00,  2.07s/it] \n",
      "[Fold 3][Epoch 9/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:32<00:00,  2.08it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7764 | Val Loss: 0.1087 | Val Acc: 96.70% | LogLoss: 0.1089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 10/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [50:03<00:00,  2.18s/it] \n",
      "[Fold 3][Epoch 10/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:35<00:00,  2.06it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7401 | Val Loss: 0.0982 | Val Acc: 97.06% | LogLoss: 0.0983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 11/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [46:47<00:00,  2.03s/it]\n",
      "[Fold 3][Epoch 11/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:29<00:00,  2.10it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7176 | Val Loss: 0.0936 | Val Acc: 97.11% | LogLoss: 0.0937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 12/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [49:50<00:00,  2.17s/it] \n",
      "[Fold 3][Epoch 12/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:09<00:00,  2.24it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7031 | Val Loss: 0.0884 | Val Acc: 97.22% | LogLoss: 0.0885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 13/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [44:13<00:00,  1.92s/it]\n",
      "[Fold 3][Epoch 13/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:09<00:00,  2.23it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6732 | Val Loss: 0.0866 | Val Acc: 97.23% | LogLoss: 0.0867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 14/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [44:01<00:00,  1.91s/it]\n",
      "[Fold 3][Epoch 14/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:08<00:00,  2.24it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6783 | Val Loss: 0.0844 | Val Acc: 97.41% | LogLoss: 0.0845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fold 3][Epoch 15/15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1381/1381 [43:51<00:00,  1.91s/it]\n",
      "[Fold 3][Epoch 15/15] Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [05:08<00:00,  2.24it/s]\n",
      "/Users/hyun/venv/torch_venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6560 | Val Loss: 0.0838 | Val Acc: 97.39% | LogLoss: 0.0839\n"
     ]
    }
   ],
   "source": [
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_logloss = checkpoint.get('best_logloss', float('inf'))\n",
    "    best_acc = checkpoint.get('best_acc', 0.0)\n",
    "    best_ce_loss = checkpoint.get('best_ce_loss', float('inf'))\n",
    "    return start_epoch, best_logloss, best_acc, best_ce_loss\n",
    "\n",
    "resume = True\n",
    "resume_ckpt = {\n",
    "    0: None,\n",
    "    1: None,\n",
    "    2: \"final_model_fold3/checkpoint_epoch_001.pth\",\n",
    "    3: None,\n",
    "    4: None\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=42)\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n",
    "    if fold != 2:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüìÇ Fold {fold+1}/{CFG['N_SPLITS']}\")\n",
    "\n",
    "    train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "    val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    model = BaseModel(num_classes=len(class_names)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'], eta_min=1e-6)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    save_dir = f\"final_model_fold{fold+1}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_logloss = float('inf')\n",
    "    best_acc = 0.0\n",
    "    best_ce_loss = float('inf')\n",
    "    if resume and resume_ckpt.get(fold):\n",
    "        print(f\"üîÑ Resuming fold {fold+1} from {resume_ckpt[fold]}\")\n",
    "        start_epoch, best_logloss, best_acc, best_ce_loss = load_checkpoint(\n",
    "            model, optimizer, scheduler, resume_ckpt[fold]\n",
    "        )\n",
    "        print(f\"Resume: start_epoch={start_epoch}, best_logloss={best_logloss}, best_acc={best_acc}, best_ce_loss={best_ce_loss}\")\n",
    "\n",
    "    for epoch in range(start_epoch, CFG['EPOCHS']):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"[Fold {fold+1}][Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            inputs, targets_a, targets_b, lam = apply_mixup_or_cutmix(\n",
    "                images, labels, mix_prob=0.5, mixup_alpha=0.2, cutmix_alpha=1.0\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"[Fold {fold+1}][Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | LogLoss: {val_logloss:.4f}\")\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_logloss': best_logloss,\n",
    "            'best_acc': best_acc,\n",
    "            'best_ce_loss': best_ce_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{save_dir}/checkpoint_epoch_{epoch+1:03d}.pth\")\n",
    "\n",
    "        if val_logloss < best_logloss:\n",
    "            best_logloss = val_logloss\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/best_logloss.pth\")\n",
    "\n",
    "        if val_accuracy > best_acc:\n",
    "            best_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/best_acc.pth\")\n",
    "\n",
    "        if avg_val_loss < best_ce_loss:\n",
    "            best_ce_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/best_loss.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset(test_root, transform=None, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, TTA 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 517/517 [03:58<00:00,  2.17it/s]\n",
      "Fold 1, TTA 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 517/517 [03:58<00:00,  2.17it/s]\n",
      "Fold 1, TTA 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 517/517 [04:05<00:00,  2.11it/s]\n",
      "Fold 2, TTA 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 517/517 [04:02<00:00,  2.13it/s]\n",
      "Fold 2, TTA 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 517/517 [04:02<00:00,  2.13it/s]\n",
      "Fold 2, TTA 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 517/517 [04:06<00:00,  2.09it/s]\n",
      "Fold 3, TTA 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 517/517 [04:07<00:00,  2.09it/s]\n",
      "Fold 3, TTA 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 517/517 [04:06<00:00,  2.10it/s]\n",
      "Fold 3, TTA 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 517/517 [04:07<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 3-Fold √ó 3-TTA soft voting ÏôÑÎ£å Î∞è Ï†ÄÏû• ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = CFG['IMG_SIZE']\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "BATCH_SIZE = CFG['BATCH_SIZE']\n",
    "\n",
    "\n",
    "def get_tta_transforms():\n",
    "    return [\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.RandomRotation(degrees=5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "    ]\n",
    "\n",
    "tta_transforms = get_tta_transforms()\n",
    "\n",
    "fold_list = [1, 2, 3]\n",
    "model_paths = [\n",
    "    f'/Users/hyun/dev_ws/dacon_car/final_model_fold{fold}/checkpoint_epoch_015.pth'\n",
    "    for fold in fold_list\n",
    "]\n",
    "\n",
    "ensemble_probs = []\n",
    "\n",
    "for fold_idx, path in enumerate(model_paths):\n",
    "    model = BaseModel(num_classes=len(class_names))\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tta_idx, tta_tf in enumerate(tta_transforms):\n",
    "            probs_list = []\n",
    "\n",
    "            for images in tqdm(test_loader, desc=f\"Fold {fold_idx+1}, TTA {tta_idx+1}\"):\n",
    "                transformed = [tta_tf(img) for img in images]\n",
    "                transformed = torch.stack(transformed).to(device)\n",
    "\n",
    "                outputs = model(transformed)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                probs_list.append(probs.cpu())\n",
    "\n",
    "            all_probs = torch.cat(probs_list, dim=0)\n",
    "            ensemble_probs.append(all_probs)\n",
    "\n",
    "avg_probs = torch.stack(ensemble_probs).mean(dim=0)\n",
    "\n",
    "results = [\n",
    "    {class_names[i]: prob[i].item() for i in range(len(class_names))}\n",
    "    for prob in avg_probs\n",
    "]\n",
    "pred = pd.DataFrame(results)\n",
    "\n",
    "pred['label'] = pred[class_names].idxmax(axis=1)\n",
    "pred['ID'] = [f'TEST_{i:05d}' for i in range(len(pred))]\n",
    "\n",
    "submission = pred[['ID', 'label']]\n",
    "submission.to_csv(\"ensemble_3fold_3tta_submission.csv\", index=False)\n",
    "print(\"‚úÖ 3-Fold √ó 3-TTA soft voting ÏôÑÎ£å Î∞è Ï†ÄÏû• ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "class_columns = submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "\n",
    "submission[class_columns] = pred.values\n",
    "submission.to_csv('test36_submission.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
